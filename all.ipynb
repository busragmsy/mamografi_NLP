{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwOthowapBv8"
   },
   "source": [
    "BİRADS KODU BURDAN BAŞLIYOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\busra\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\busra\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\busra\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\busra\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\busra\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\busra\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDsaYCNDKdlX",
    "outputId": "d4bd5bbf-157d-4c8d-b4ec-9727f9032d1d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "\n",
    "# Metin Temizleme Fonksiyonu\n",
    "def metin_temizle(metin):\n",
    "    metin = metin.lower()\n",
    "    metin = re.sub(r'[^a-zçğıöşü\\s]', '', metin)\n",
    "    metin = re.sub(r'\\s+', ' ', metin).strip()\n",
    "    metin = ' '.join([kelime for kelime in metin.split() if kelime not in stop_words])\n",
    "    return metin\n",
    "\n",
    "# Dosya Okuma Fonksiyonu\n",
    "def dosyalari_oku(dosya_yolu):\n",
    "    try:\n",
    "        with open(dosya_yolu, 'r', encoding='utf-8') as file:\n",
    "            raporlar = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(dosya_yolu, 'r', encoding='ISO-8859-1') as file:\n",
    "            raporlar = file.read()\n",
    "    return raporlar\n",
    "\n",
    "# Dosyaları Ayırma Fonksiyonu\n",
    "def dosyalari_ayir(dizin_yolu, train_count, val_count, test_count):\n",
    "    dosyalar = os.listdir(dizin_yolu)\n",
    "    np.random.shuffle(dosyalar)\n",
    "\n",
    "    train_dosyalar = dosyalar[:train_count]\n",
    "    val_dosyalar = dosyalar[train_count:train_count + val_count]\n",
    "    test_dosyalar = dosyalar[train_count + val_count:train_count + val_count + test_count]\n",
    "\n",
    "    return train_dosyalar, val_dosyalar, test_dosyalar\n",
    "\n",
    "# BIRADS Klasör Yolları\n",
    "birads_klasor_yollari = [\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/2000raporBIRADSgruplanmış/BIRADS1\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/2000raporBIRADSgruplanmış/BIRADS2\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/2000raporBIRADSgruplanmış/BIRADS3\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/2000raporBIRADSgruplanmış/BIRADS4\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/2000raporBIRADSgruplanmış/BIRADS5\"\n",
    "]\n",
    "# Sınıf başına veri sayıları\n",
    "birads_sayilari = [405, 518, 615, 721, 759]\n",
    "\n",
    "# Eğitim, doğrulama ve test oranları\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Eğitim, doğrulama ve test setleri\n",
    "X_train_raporlar = []\n",
    "X_val_raporlar = []\n",
    "X_test_raporlar = []\n",
    "y_train = []\n",
    "y_val = []\n",
    "y_test = []\n",
    "\n",
    "# Dosyaları sınıflara göre ayırma ve okuma\n",
    "for i, dizin_yolu in enumerate(birads_klasor_yollari):\n",
    "    birads_sayi = birads_sayilari[i]\n",
    "    train_count = int(birads_sayi * train_ratio)\n",
    "    val_count = int(birads_sayi * val_ratio)\n",
    "    test_count = birads_sayi - train_count - val_count\n",
    "\n",
    "    train_files, val_files, test_files = dosyalari_ayir(dizin_yolu, train_count, val_count, test_count)\n",
    "\n",
    "    # Eğitim seti için\n",
    "    for dosya in train_files:\n",
    "        rapor = dosyalari_oku(os.path.join(dizin_yolu, dosya))\n",
    "        X_train_raporlar.append(metin_temizle(rapor))\n",
    "        y_train.append(i + 1)\n",
    "\n",
    "    # Doğrulama seti için\n",
    "    for dosya in val_files:\n",
    "        rapor = dosyalari_oku(os.path.join(dizin_yolu, dosya))\n",
    "        X_val_raporlar.append(metin_temizle(rapor))\n",
    "        y_val.append(i + 1)\n",
    "\n",
    "    # Test seti için\n",
    "    for dosya in test_files:\n",
    "        rapor = dosyalari_oku(os.path.join(dizin_yolu, dosya))\n",
    "        X_test_raporlar.append(metin_temizle(rapor))\n",
    "        y_test.append(i + 1)\n",
    "\n",
    "# TF-IDF Vektörleştirme\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_raporlar)\n",
    "X_val_tfidf = vectorizer.transform(X_val_raporlar)\n",
    "X_test_tfidf = vectorizer.transform(X_test_raporlar)\n",
    "\n",
    "# SMOTE uygulama (yalnızca eğitim setine)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pk39z7hIKmjk",
    "outputId": "40b1356f-d146-45d8-c964-280ba0145a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğrulama Seti Accuracy: 0.8494983277591973\n",
      "Doğrulama Seti Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.95      0.90        40\n",
      "           2       0.69      0.78      0.73        51\n",
      "           3       0.86      0.80      0.83        61\n",
      "           4       0.97      0.78      0.86        72\n",
      "           5       0.87      0.95      0.90        75\n",
      "\n",
      "    accuracy                           0.85       299\n",
      "   macro avg       0.85      0.85      0.85       299\n",
      "weighted avg       0.86      0.85      0.85       299\n",
      "\n",
      "Test Seti Accuracy: 0.8333333333333334\n",
      "Test Seti Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.95      0.94        41\n",
      "           2       0.67      0.74      0.70        53\n",
      "           3       0.82      0.81      0.81        62\n",
      "           4       0.87      0.75      0.81        73\n",
      "           5       0.88      0.94      0.91        77\n",
      "\n",
      "    accuracy                           0.83       306\n",
      "   macro avg       0.83      0.84      0.83       306\n",
      "weighted avg       0.84      0.83      0.83       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Eğitimi ve Değerlendirme\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Doğrulama seti değerlendirmesi\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Doğrulama Seti Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Doğrulama Seti Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Test seti değerlendirmesi\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "print(\"Test Seti Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test Seti Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# JSON dosyasını yükleme\n",
    "def json_dosyasi_oku(json_dosya_yolu):\n",
    "    with open(json_dosya_yolu, 'r', encoding='utf-8') as file:\n",
    "        veri = json.load(file)\n",
    "    return veri\n",
    "\n",
    "# JSON dosyasına yazma\n",
    "def json_dosyasi_yaz(json_dosya_yolu, veri):\n",
    "    with open(json_dosya_yolu, 'w', encoding='utf-8') as file:\n",
    "        json.dump(veri, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Test raporlarını tahmin etme ve sonuçları JSON'a yazma\n",
    "def birads_tahmini_yap(json_dosya_yolu, model, vectorizer):\n",
    "    # JSON dosyasını oku\n",
    "    veri = json_dosyasi_oku(json_dosya_yolu)\n",
    "\n",
    "    # Her raporu temizle ve tahmin\n",
    "    for rapor in veri['tahminler']:\n",
    "        metin = metin_temizle(rapor['text'])  # Raporu temizle\n",
    "        tfidf_metin = vectorizer.transform([metin])  # TF-IDF dönüşümü\n",
    "        tahmin = model.predict(tfidf_metin)  # Model ile tahmin yap\n",
    "        rapor['cats'] = int(tahmin[0])  # Tahmini 'cats' kısmına ekle\n",
    "\n",
    "    # Yeni JSON dosyasına yazma\n",
    "    json_dosyasi_yaz('tahminli_' + os.path.basename(json_dosya_yolu), veri)\n",
    "\n",
    "# JSON dosyası ile model ve vectorizer'ı kullanarak tahmin\n",
    "json_dosya_yolu = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/nlpFİNAL.json'\n",
    "birads_tahmini_yap(json_dosya_yolu, model, vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw6lnQaRpYR-"
   },
   "source": [
    "VARLIK İSMİ ÇIKARIMI BURDAN BAŞLIYOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9z8TLoffYYx",
    "outputId": "b763dc21-b873-4f68-9c6e-2097b3de35c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosyalar başarıyla C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all.jsonl konumunda birleştirildi ve kontrol edildi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_paths = [\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/1/all.jsonl\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/2/all.jsonl\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/3/all.jsonl\",\n",
    "    \"C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/4/all.jsonl\"\n",
    "]\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Satırı JSON olarak ayrıştırma ve listeye ekleme\n",
    "                data = json.loads(line)\n",
    "                combined_data.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Geçersiz JSON satırı atlandı: {line}\")\n",
    "\n",
    "# Birleştirilmiş veriyi yeni bir JSONL dosyasına yazdırma\n",
    "output_file_path = \"C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all.jsonl\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for entry in combined_data:\n",
    "        # Her bir JSON nesnesini satır olarak yazma\n",
    "        output_file.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Dosyalar başarıyla {output_file_path} konumunda birleştirildi ve kontrol edildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztBA51JI2Ze7",
    "outputId": "9524c86d-ccec-4e4d-f79f-54a09562fcf8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))  # Her satırı JSON olarak yükle ve listeye ekle\n",
    "\n",
    "# Entite etiketlerini görselleştirme\n",
    "def visualize_labeled_text(entry):\n",
    "    labeled_text = entry['text'] \n",
    "    output_text = \"\"\n",
    "    labels = entry['label']\n",
    "\n",
    "    # \"SONUÇ\" veya \"Sonuç\" veya \"SONUC\" ile başlayan metni çıkartmak için kontrol\n",
    "    if any(sonuc in labeled_text for sonuc in [\"SONUÇ\", \"Sonuç\", \"SONUC\"]):\n",
    "        for sonuc in [\"SONUÇ\", \"Sonuç\", \"SONUC\"]:\n",
    "            if sonuc in labeled_text:\n",
    "                labeled_text = labeled_text.split(sonuc)[0]\n",
    "\n",
    "    last_index = 0\n",
    "    combined_labels = []\n",
    "\n",
    "    # Aynı türdeki entiteleri birleştirmek için güncellenmiş yaklaşım\n",
    "    for start, end, label in labels:\n",
    "        # IMPRESSION etiketlerini atla\n",
    "        if label == \"IMPRESSION\":\n",
    "            continue\n",
    "\n",
    "        # Eğer önceki entite ile devamlılık varsa ve türler aynı ise, entiteleri birleştir\n",
    "        if combined_labels and combined_labels[-1][2] == label:\n",
    "            # Aynı entite türü için mevcut birleşik entitenin bitişi ile yenisinin başlangıcı arasında boşluk, noktalama, vb. olabilir\n",
    "            combined_labels[-1] = (combined_labels[-1][0], max(combined_labels[-1][1], end), label)\n",
    "        else:\n",
    "            combined_labels.append((start, end, label))\n",
    "\n",
    "    # Birleştirilmiş etiketlerle metin oluşturma\n",
    "    for start, end, label in combined_labels:\n",
    "        # Etiketli metin kısmını oluşturma\n",
    "        output_text += labeled_text[last_index:start]  \n",
    "        output_text += f\"{labeled_text[start:end]} [{label}]\"  \n",
    "        last_index = end\n",
    "\n",
    "    # Etiketlerden sonraki kalan metni ekleme\n",
    "    output_text += labeled_text[last_index:]\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Her bir örnek için etiketli metinleri görüntüleme\n",
    "for entry in data:\n",
    "    print(f\"Metin ID: {entry['id']}\")\n",
    "    labeled_text = visualize_labeled_text(entry)\n",
    "    print(labeled_text)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rADxnXoZA_rw",
    "outputId": "8136bec8-3e07-4623-e51a-73e30e0e4f6c"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrjBitj0DUKY",
    "outputId": "d3874069-3263-470d-9d50-63d01344a08c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temizlenmiş veri dosyası kaydedildi: C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all_cleaned.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Etiketlenmiş veri dosyasını oku\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Etiketleri temizleme ve boşlukları kaldırma\n",
    "def clean_labels(data):\n",
    "    cleaned_data = []\n",
    "    for entry in data:\n",
    "        text = entry['text']\n",
    "        cleaned_labels = []\n",
    "        for start, end, label in entry['label']:\n",
    "            entity = text[start:end]\n",
    "            # Öncül ve ardıl boşlukları kaldırma\n",
    "            new_entity = entity.strip()\n",
    "            new_start = start + (len(entity) - len(entity.lstrip()))\n",
    "            new_end = end - (len(entity) - len(entity.rstrip()))\n",
    "            cleaned_labels.append((new_start, new_end, label))\n",
    "        cleaned_data.append({'id': entry['id'], 'text': text, 'label': cleaned_labels})\n",
    "    return cleaned_data\n",
    "\n",
    "# Güncellenmiş veriyi kaydetme\n",
    "def save_cleaned_data(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def main():\n",
    "    file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all.jsonl'\n",
    "    cleaned_file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all_cleaned.jsonl'\n",
    "\n",
    "    data = load_data(file_path)\n",
    "    cleaned_data = clean_labels(data)\n",
    "\n",
    "    save_cleaned_data(cleaned_file_path, cleaned_data)\n",
    "    print(\"Temizlenmiş veri dosyası kaydedildi:\", cleaned_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrhgxuyaDheT",
    "outputId": "371415fe-87b4-4ccf-8a92-8a7e113801fb"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "def load_cleaned_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Veri hazırlama\n",
    "def prepare_data(data):\n",
    "    examples = []\n",
    "    for entry in data:\n",
    "        text = entry['text']\n",
    "        ents = [(start, end, label) for start, end, label in entry['label']]\n",
    "        examples.append((text, {\"entities\": ents}))\n",
    "    return examples\n",
    "\n",
    "# Model eğitimi\n",
    "def train_ner_model(train_data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "    # Etiketleri ekle\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Eğitici oluştur ve eğitime başla\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(20):  # Eğitim döngüsü sayısı\n",
    "        losses = {}\n",
    "        for text, annotations in train_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses, drop=0.35)\n",
    "        print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "    # Eğitilmiş modeli kaydet\n",
    "    nlp.to_disk(\"ner_model\")\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    cleaned_file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all_cleaned.jsonl'\n",
    "\n",
    "    data = load_cleaned_data(cleaned_file_path)\n",
    "    train_data = prepare_data(data)\n",
    "    train_ner_model(train_data)\n",
    "    print(\"Model eğitildi ve kaydedildi.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "-_4AntLFJDyJ",
    "outputId": "62552e33-409a-48cd-b8e3-083322784c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Çıkarılan varlıklar: [('bir metin', 'OBS-PRESENT'), ('burada varlık çıkarımı', 'OBS-PRESENT')]\n",
      "Precision: 0.77\n",
      "Recall: 0.71\n",
      "F1-Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "# Eğitilmiş modeli yükle\n",
    "def load_model(model_path):\n",
    "    nlp = spacy.load(model_path)\n",
    "    return nlp\n",
    "\n",
    "# Metin üzerinde varlık çıkarımı yap\n",
    "def predict_entities(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Veri hazırlama (Example nesneleri oluşturma)\n",
    "def prepare_examples(data, nlp):\n",
    "    examples = []\n",
    "    for entry in data:\n",
    "        text = entry['text']\n",
    "        ents = [(start, end, label) for start, end, label in entry['label']]\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, {\"entities\": ents})\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# Modeli değerlendir\n",
    "def evaluate_model(nlp, data):\n",
    "    examples = prepare_examples(data, nlp)\n",
    "    scorer = nlp.evaluate(examples)\n",
    "    print(f\"Precision: {scorer['ents_p']:.2f}\")\n",
    "    print(f\"Recall: {scorer['ents_r']:.2f}\")\n",
    "    print(f\"F1-Score: {scorer['ents_f']:.2f}\")\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    model_path = \"ner_model\"\n",
    "    nlp = load_model(model_path)\n",
    "\n",
    "    # Test metni\n",
    "    test_text = \"Örnek bir metin, burada varlık çıkarımı yapılacak.\"\n",
    "    entities = predict_entities(nlp, test_text)\n",
    "    print(\"Çıkarılan varlıklar:\", entities)\n",
    "\n",
    "    # Modeli değerlendirme\n",
    "    cleaned_file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/NLPJSON/combined_all_cleaned.jsonl'\n",
    "    data = load_cleaned_data(cleaned_file_path)\n",
    "    evaluate_model(nlp, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varlık çıkarımı tamamlandı ve sonuçlar C:/Users/BUSRA/Documents/NLP-Teknofest24/tahminli_nlp_demo_dataset_with_entities_merged.json dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Eğitilmiş modeli yükle\n",
    "def load_model(model_path):\n",
    "    nlp = spacy.load(model_path)\n",
    "    return nlp\n",
    "\n",
    "# Metin üzerinde varlık çıkarımı yap\n",
    "def predict_entities(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Aynı türdeki bitişik varlıkları birleştir\n",
    "def merge_adjacent_entities(entities):\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    merged_entities = []\n",
    "    current_start, current_end, current_label = entities[0]\n",
    "    \n",
    "    for start, end, label in entities[1:]:\n",
    "        # Ardışık ve aynı etiketli ve arada boşluk/bağlaç varsa\n",
    "        if label == current_label and start <= current_end + 1:  \n",
    "            current_end = end  # Bitiş noktasını güncelle\n",
    "        else:\n",
    "            merged_entities.append((current_start, current_end, current_label))\n",
    "            current_start, current_end, current_label = start, end, label\n",
    "    \n",
    "    # Son kalan varlığı ekle\n",
    "    merged_entities.append((current_start, current_end, current_label))\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "# Test dosyasını yükle\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Test dosyasını işle ve varlık çıkarım sonuçlarını entities anahtarına ekle\n",
    "def process_test_data(nlp, test_data):\n",
    "    for entry in test_data['tahminler']:  # \"tahminler\" altındaki verileri işliyoruz\n",
    "        text = entry.get('text', '')  # Her entry'den text'i alın\n",
    "        if text:\n",
    "            entities = predict_entities(nlp, text)  # Model ile varlık çıkarımı yap\n",
    "            merged_entities = merge_adjacent_entities(entities)  # Aynı etiketli bitişik varlıkları birleştir\n",
    "            entry['entities'] = merged_entities  # Birleştirilmiş varlıkları entities kısmına yaz\n",
    "    return test_data\n",
    "\n",
    "# Sonuçları yeni bir dosyaya kaydet\n",
    "def save_results(output_file_path, data):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    model_path = \"ner_model\"  # Eğitilmiş model yolu\n",
    "    test_file_path = \"C:/Users/BUSRA/Documents/NLP-Teknofest24/nlp_test_dataset (2)/nlp_test_dataset.json\"  # Test dosyanızın yolu\n",
    "    output_file_path = \"C:/Users/BUSRA/Documents/NLP-Teknofest24/tahminli_nlp_demo_dataset_with_entities_merged.json\"  # Sonuçların kaydedileceği dosya\n",
    "\n",
    "    # Modeli yükle\n",
    "    nlp = load_model(model_path)\n",
    "\n",
    "    # Test verilerini yükle\n",
    "    test_data = load_test_data(test_file_path)\n",
    "\n",
    "    # Test verilerini işle ve varlık çıkarımı yap\n",
    "    processed_data = process_test_data(nlp, test_data)\n",
    "\n",
    "    # Sonuçları yeni bir dosyaya kaydet\n",
    "    save_results(output_file_path, processed_data)\n",
    "\n",
    "    print(f\"Varlık çıkarımı tamamlandı ve sonuçlar {output_file_path} dosyasına kaydedildi.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/BUSRA/Documents/NLP-Teknofest24/nlpFİNAL.json'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/tahminli_nlp_demo_dataset_with_entities_merged.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update the \"kunye\" section\n",
    "data['kunye']['takim_adi'] = \"Diagnos-ai\"\n",
    "data['kunye']['takim_id'] = \"344193\"\n",
    "data['kunye']['aciklama'] = \"ikinci aşama çıktısı\"\n",
    "\n",
    "# Save the updated JSON back to a new file\n",
    "output_file_path = 'C:/Users/BUSRA/Documents/NLP-Teknofest24/nlpFİNAL.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity extraction complete. Results saved to C:/Users/BUSRA/Documents/NLP-Teknofest24/tahminli_nlp_demo_dataset_with_entities_merged.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model_path):\n",
    "    nlp = spacy.load(model_path)\n",
    "    return nlp\n",
    "\n",
    "# Perform entity recognition on text\n",
    "def predict_entities(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Merge adjacent entities with the same label\n",
    "def merge_adjacent_entities(entities):\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    merged_entities = []\n",
    "    current_start, current_end, current_label = entities[0]\n",
    "    \n",
    "    for start, end, label in entities[1:]:\n",
    "        # If consecutive and same-labeled entities are adjacent\n",
    "        if label == current_label and start <= current_end + 1:  \n",
    "            current_end = end  # Update the end position\n",
    "        else:\n",
    "            merged_entities.append((current_start, current_end, current_label))\n",
    "            current_start, current_end, current_label = start, end, label\n",
    "    \n",
    "    # Append the last remaining entity\n",
    "    merged_entities.append((current_start, current_end, current_label))\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "# Load the test data file\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Process the test data and add entity extraction results to the \"entities\" field\n",
    "def process_test_data(nlp, test_data):\n",
    "    for entry in test_data['tahminler']:  # Process entries under \"tahminler\"\n",
    "        text = entry.get('text', '')  # Get the text for each entry\n",
    "        if text:\n",
    "            entities = predict_entities(nlp, text)  # Extract entities with the model\n",
    "            merged_entities = merge_adjacent_entities(entities)  # Merge adjacent entities of the same label\n",
    "            \n",
    "            # Format entities according to the example in the image\n",
    "            formatted_entities = [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in merged_entities]\n",
    "            entry['entities'] = formatted_entities  # Assign formatted entities back to the entry\n",
    "\n",
    "    return test_data\n",
    "\n",
    "# Save results to a new file\n",
    "def save_results(output_file_path, data):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    model_path = \"ner_model\"  # Path to your trained model\n",
    "    test_file_path = \"C:/Users/BUSRA/Documents/NLP-Teknofest24/nlp_demo_dataset.json\" # Path to your test dataset\n",
    "    output_file_path = \"C:/Users/BUSRA/Documents/NLP-Teknofest24/tahminli_nlp_demo_dataset_with_entities_merged.json\"  # Path to save results\n",
    "\n",
    "    # Load the model\n",
    "    nlp = load_model(model_path)\n",
    "\n",
    "    # Load the test data\n",
    "    test_data = load_test_data(test_file_path)\n",
    "\n",
    "    # Process test data and perform entity extraction\n",
    "    processed_data = process_test_data(nlp, test_data)\n",
    "\n",
    "    # Save the results to a new file\n",
    "    save_results(output_file_path, processed_data)\n",
    "\n",
    "    print(f\"Entity extraction complete. Results saved to {output_file_path}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
